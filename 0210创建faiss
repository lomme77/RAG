from langchain_community.document_loaders import HuggingFaceDatasetLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers import AutoTokenizer, pipeline
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.chains import RetrievalQA

# 指定数据集名称和包含内容的列
dataset_name = "databricks/databricks-dolly-15k"
page_content_column = "context"  # 或者其他您感兴趣的列

# 创建加载器实例
loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)

# 加载数据
data = loader.load()

# 显示前15个条目
# 使用特定参数创建RecursiveCharacterTextSplitter类的实例。
# 它将文本分成每个1000个字符的块，每个块有150个字符的重叠。
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

# 'data'包含您想要拆分的文本，使用文本拆分器将文本拆分为文档。
docs = text_splitter.split_documents(data)
docs[0]

# 定义要使用的预训练模型的路径
modelPath = "sentence-transformers/all-MiniLM-l6-v2"

# 创建一个包含模型配置选项的字典，指定使用CPU进行计算
model_kwargs = {'device':'cpu'}

# 创建一个包含编码选项的字典，具体设置 'normalize_embeddings' 为 False
encode_kwargs = {'normalize_embeddings': False}

# 使用指定的参数初始化HuggingFaceEmbeddings的实例
embeddings = HuggingFaceEmbeddings(
    model_name=modelPath,     # 提供预训练模型的路径
    model_kwargs=model_kwargs, # 传递模型配置选项
    encode_kwargs=encode_kwargs # 传递编码选项
)
text = "This is a Golden Retrievers"
query_result = embeddings.embed_query(text)
query_result[:3]

db = FAISS.from_documents(docs, embeddings)
db.save_local("0210faiss_index")